{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from math import radians, cos, sin, asin, sqrt, atan2, degrees\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([('spark.executor.memory', '8g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g')])\n",
    "\n",
    "sc = SparkContext(\"local\", \"Simple App\", conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.port', '65419'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.cores.max', '3'),\n",
       " ('spark.app.id', 'local-1551988008103'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', 'zis-mbp-2'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.executor.cores', '3'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'Simple App'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure directories for source data files and log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_1_file = 'xian_sample_1.csv'\n",
    "full_data_1_file = 'gps_20161001'\n",
    "sample_dir = '../data/'\n",
    "full_data_dir = '../data_full/xian/'\n",
    "output_dir = '../output/'\n",
    "boundary_path = '../log/boundary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file = sample_dir + sample_data_1_file\n",
    "data_file = full_data_dir + full_data_1_file\n",
    "output_file = output_dir + full_data_1_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "SHOWLINES = 5\n",
    "TIMEFORMAT = \"yyyy-MM-dd HH:mm:ss\"\n",
    "grid_x_size = 1 # Grid with horizontal length of 1 km\n",
    "grid_y_size = 1 # Grid with vertical length of 1 km\n",
    "time_interval = 600 # Unit: sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df(df):\n",
    "    if DEBUG:\n",
    "        print(df.show(SHOWLINES))\n",
    "        df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(longit_a, latit_a, longit_b, latit_b):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "        https://medium.com/@nikolasbielski/using-a-custom-udf-in-pyspark-to-compute-haversine-distances-d877b77b4b18\n",
    "    Return:\n",
    "        double(,3): unit in km\n",
    "    \"\"\" \n",
    "    # Transform to radians\n",
    "    longit_a, latit_a, longit_b, latit_b = map(radians, [longit_a,  latit_a, longit_b, latit_b])\n",
    "    dist_longit = longit_b - longit_a\n",
    "    dist_latit = latit_b - latit_a\n",
    "    # Calculate area\n",
    "    area = sin(dist_latit/2)**2 + cos(latit_a) * sin(dist_longit/2)**2\n",
    "    # Calculate the central angle\n",
    "    central_angle = 2 * asin(sqrt(area))\n",
    "    radius = 6371\n",
    "    # Calculate Distance, unit = km\n",
    "    distance = central_angle * radius\n",
    "    return abs(round(distance, 3))\n",
    "\n",
    "udf_get_distance = F.udf(get_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity(distance, time):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "        double(,3): unit in km/h\n",
    "    \"\"\" \n",
    "    return(round(distance/time*3600, 3))\n",
    "\n",
    "udf_get_velocity = F.udf(get_velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(longit_a, latit_a, longit_b, latit_b):\n",
    "    \"\"\"\n",
    "    Reference: https://gist.github.com/jeromer/2005586\n",
    "\n",
    "    Return:\n",
    "        (0,1,2,3) => \"S, W, N, E\"\n",
    "        -1 => Stationary\n",
    "    \"\"\"\n",
    "    diffLong = longit_b - longit_a\n",
    "    \n",
    "    if diffLong == 0:\n",
    "        return -1\n",
    "        \n",
    "    x = sin(diffLong) * cos(latit_b)\n",
    "    y = cos(latit_a) * sin(latit_b) - (sin(latit_a)\n",
    "            * cos(latit_b) * cos(diffLong))\n",
    "\n",
    "    initial_bearing = atan2(x, y)\n",
    "\n",
    "    # Now we have the initial bearing but math.atan2 return values\n",
    "    # from -180° to + 180° which is not what we want for a compass bearing\n",
    "    # The solution is to normalize the initial bearing as shown below\n",
    "    initial_bearing = degrees(initial_bearing)\n",
    "#     compass_bearing = (initial_bearing + 360) % 360\n",
    "\n",
    "#     return round(compass_bearing, 3)\n",
    "\n",
    "    # Version 2: Return (0,1,2,3) => \"S, W, N, E\"\n",
    "    # First make the initial bearing positive, which is from 0° to 360°\n",
    "    # Then add a 45° offset to get the approx. direction\n",
    "    direction = (initial_bearing + 180 + 45) % 360 // 90\n",
    "    return round(direction, 0)\n",
    "\n",
    "udf_get_direction = F.udf(get_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_interval(lat_max, lat_min, lon_max, lon_min, x_size, y_size):\n",
    "    # Get width and length\n",
    "    total_x_xian = get_distance(lon_max, lat_max, lon_min, lat_max)\n",
    "    total_y_xian = get_distance(lon_max, lat_max, lon_max, lat_min)\n",
    "    \n",
    "    # Get number of grids\n",
    "    num_grid_x = total_x_xian//grid_x_size + 1\n",
    "    num_grid_y = total_y_xian//grid_y_size + 1\n",
    "    \n",
    "    # Get interval in coordinates\n",
    "    lon_interval = round((lon_max - lon_min)/num_grid_x, 6)\n",
    "    lat_interval = round((lat_max - lat_min)/num_grid_y, 6)\n",
    "    \n",
    "    return num_grid_x, num_grid_y, lon_interval, lat_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_coord(x, x_min, interval):\n",
    "    coord_index = max((x - x_min)//interval, 0)\n",
    "    return coord_index\n",
    "\n",
    "udf_get_grid_coord = F.udf(get_grid_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import data from csv using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Didi data with pre-defined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [StructField('driver_id', StringType(), True), \n",
    "          StructField('order_id', StringType(), True),\n",
    "         StructField('timestamp', IntegerType(), True),\n",
    "         StructField('lon', DoubleType(), True),\n",
    "         StructField('lat', DoubleType(), True)]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_df = sqlContext.read.csv(data_file, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_df(didi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert unixtime to proper timestamp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# didi_df_1 = didi_df.withColumn('timestamp', F.from_unixtime(\"timestamp\"))\n",
    "\n",
    "# Removed timestamp conversion\n",
    "# It's easier to compute time difference using the original integer format\n",
    "didi_df_1 = didi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(didi_df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create index for each order_id, sorted by timestamp in descending order. <br/>\n",
    "This is to identify nodes at time t and t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(didi_df_1['order_id']).orderBy(didi_df_1['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_df_2 = didi_df_1.withColumn('rank', F.dense_rank().over(window))\n",
    "check_df(didi_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a duplicate for t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_df_2_post = didi_df_2.\\\n",
    "    withColumnRenamed(\"rank\", \"rank_new\").\\\n",
    "    withColumnRenamed(\"timestamp\", \"timestamp_new\").\\\n",
    "    withColumnRenamed(\"lon\", \"lon_new\").\\\n",
    "    withColumnRenamed(\"lat\", \"lat_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_df_2_post = didi_df_2_post.drop('driver_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(didi_df_2_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the two tables for t and t+1 on order ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_df_3 = didi_df_2.alias('a').join(didi_df_2_post.alias('b'),on = 'order_id').where('a.rank == b.rank_new-1')\n",
    "check_df(didi_df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get distance using coordinates of $node_t$ and $node_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "didi_df_4 = didi_df_3.withColumn('distance', udf_get_distance(\n",
    "    didi_df_3.lon, didi_df_3.lat, \n",
    "    didi_df_3.lon_new, didi_df_3.lat_new).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(didi_df_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get time difference between $node_t$ and $node_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeDiff = (F.unix_timestamp('timestamp_new', format=TIMEFORMAT)\n",
    "#             - F.unix_timestamp('timestamp', format=TIMEFORMAT))\n",
    "timeDiff = didi_df_4.timestamp_new - didi_df_4.timestamp\n",
    "didi_df_5 = didi_df_4.withColumn(\"eclipse_time\", timeDiff)\n",
    "check_df(didi_df_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get velocity between $node_t$ and $node_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_df_6 = didi_df_5.withColumn(\"velocity\", udf_get_velocity(\n",
    "    didi_df_5.distance, didi_df_5.eclipse_time).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(didi_df_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get compass bearing when travelling from $node_t$ to $node_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_df_7 = didi_df_6.withColumn('direction', udf_get_direction(\n",
    "    didi_df_6.lon, didi_df_6.lat, \n",
    "    didi_df_6.lon_new, didi_df_6.lat_new).cast(DoubleType()))\n",
    "check_df(didi_df_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construct the Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the boundaries from existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>time_max</th>\n",
       "      <th>time_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/xian_sample_1.csv</td>\n",
       "      <td>03/08/2019 00:13:26</td>\n",
       "      <td>34.27678</td>\n",
       "      <td>34.21653</td>\n",
       "      <td>108.98842</td>\n",
       "      <td>108.92246</td>\n",
       "      <td>2016-10-01 23:13:11</td>\n",
       "      <td>2016-10-01 07:17:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/xian_sample_2.csv</td>\n",
       "      <td>03/08/2019 00:13:27</td>\n",
       "      <td>34.23308</td>\n",
       "      <td>34.21647</td>\n",
       "      <td>108.93969</td>\n",
       "      <td>108.91170</td>\n",
       "      <td>2016-10-02 05:52:25</td>\n",
       "      <td>2016-10-02 05:47:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data_full/xian/gps_20161002</td>\n",
       "      <td>03/08/2019 00:19:22</td>\n",
       "      <td>34.28017</td>\n",
       "      <td>34.20531</td>\n",
       "      <td>108.99860</td>\n",
       "      <td>108.91119</td>\n",
       "      <td>2016-10-03 00:01:11</td>\n",
       "      <td>2016-10-02 00:01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data_full/xian/gps_20161001</td>\n",
       "      <td>03/08/2019 00:22:49</td>\n",
       "      <td>34.28022</td>\n",
       "      <td>34.20531</td>\n",
       "      <td>108.99860</td>\n",
       "      <td>108.91118</td>\n",
       "      <td>2016-10-02 00:01:10</td>\n",
       "      <td>2016-10-01 00:02:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename       execution_time   lat_max   lat_min  \\\n",
       "0       ../data/xian_sample_1.csv  03/08/2019 00:13:26  34.27678  34.21653   \n",
       "1       ../data/xian_sample_2.csv  03/08/2019 00:13:27  34.23308  34.21647   \n",
       "2  ../data_full/xian/gps_20161002  03/08/2019 00:19:22  34.28017  34.20531   \n",
       "3  ../data_full/xian/gps_20161001  03/08/2019 00:22:49  34.28022  34.20531   \n",
       "\n",
       "     lon_max    lon_min             time_max             time_min  \n",
       "0  108.98842  108.92246  2016-10-01 23:13:11  2016-10-01 07:17:26  \n",
       "1  108.93969  108.91170  2016-10-02 05:52:25  2016-10-02 05:47:09  \n",
       "2  108.99860  108.91119  2016-10-03 00:01:11  2016-10-02 00:01:01  \n",
       "3  108.99860  108.91118  2016-10-02 00:01:10  2016-10-01 00:02:12  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundary_df = pd.read_csv(boundary_path, skipinitialspace=True, index_col=False)\n",
    "boundary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.28022 34.20531 108.9986 108.91118 2016-10-03 00:01:11 2016-10-01 00:02:12\n"
     ]
    }
   ],
   "source": [
    "lat_max = boundary_df[\"lat_max\"].max()\n",
    "lat_min = boundary_df[\"lat_min\"].min()\n",
    "lon_max = boundary_df[\"lon_max\"].max()\n",
    "lon_min = boundary_df[\"lon_min\"].min()\n",
    "time_max = boundary_df[\"time_max\"].max()\n",
    "time_min = boundary_df[\"time_min\"].min()\n",
    "print(lat_max, lat_min, lon_max, lon_min, time_max, time_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475251332.0\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp format to unix time\n",
    "time_min = datetime.strptime(time_min, \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "print(time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grid_x, num_grid_y, lon_interval, lat_interval = get_grid_interval(\n",
    "    lat_max, lat_min, lon_max, lon_min, grid_x_size, grid_y_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_df_8 = didi_df_7.withColumn('lat_min', F.lit(lat_min)).\\\n",
    "    withColumn('lon_min', F.lit(lon_min)).\\\n",
    "    withColumn('time_min', F.lit(time_min)).\\\n",
    "    withColumn('lat_interval', F.lit(lat_interval)).\\\n",
    "    withColumn('lon_interval', F.lit(lon_interval)).\\\n",
    "    withColumn('time_interval', F.lit(time_interval))    \n",
    "check_df(didi_df_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_df_9 = didi_df_8.withColumn('x', udf_get_grid_coord(didi_df_8.lon, didi_df_8.lon_min, didi_df_8.lon_interval)\n",
    "                             .cast(IntegerType())).\\\n",
    "    withColumn('y', udf_get_grid_coord(didi_df_8.lat, didi_df_8.lat_min, didi_df_8.lat_interval)\n",
    "                             .cast(IntegerType())).\\\n",
    "    withColumn('t', udf_get_grid_coord(didi_df_8.timestamp, didi_df_8.time_min, didi_df_8.time_interval)\n",
    "                             .cast(IntegerType()))\n",
    "check_df(didi_df_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregate for the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions of a grid:\n",
    "- x\n",
    "- y\n",
    "- t\n",
    "- direction:\n",
    "    - -1: Stationary\n",
    "    - 0: S\n",
    "    - 1: W\n",
    "    - 2: N\n",
    "    - 3: E\n",
    "\n",
    "\n",
    "Metrics of a grid:\n",
    "- Number of nodes/instances\n",
    "- Average velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_aggr_1 = didi_df_9.groupBy(\"x\", \"y\", \"t\", \"direction\").agg({\n",
    "    \"order_id\": 'count', 'velocity': 'avg'\n",
    "})\n",
    "check_df(didi_aggr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_aggr_2 = didi_aggr_1.withColumnRenamed(\"avg(velocity)\", \"avg_velocity\").\\\n",
    "    withColumnRenamed(\"count(order_id)\", \"cnt_nodes\")\n",
    "check_df(didi_aggr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write to output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---------+------------------+---------+\n",
      "|  x|  y|  t|direction|      avg_velocity|cnt_nodes|\n",
      "+---+---+---+---------+------------------+---------+\n",
      "|  3|  7|130|     -1.0|               0.0|        1|\n",
      "|  7|  1|139|      1.0|              35.6|        3|\n",
      "|  1|  8| 43|      1.0|48.934714285714286|       14|\n",
      "|  7|  1|139|      3.0|57.900000000000006|        6|\n",
      "|  7|  1|139|      2.0|              48.6|        2|\n",
      "+---+---+---+---------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "didi_aggr_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "didi_aggr_2.write.csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back from saved files\n",
    "# didi_aggr_3 = sqlContext.read.csv(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
